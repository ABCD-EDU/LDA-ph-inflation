{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "# for text preprocessing\n",
    "import re\n",
    "# import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "dataframe = pd.read_csv(\"./data/preprocessed.csv\")\n",
    "# clean_corpus = dataframe[\"preprocessed\"]\n",
    "# clean_corpus = [x.strip('][').split(', ') for x in dataframe[\"preprocessed\"]]\n",
    "# string to list\n",
    "clean_corpus = [ast.literal_eval(x) for x in dataframe[\"preprocessed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where every unique term is assigned an index.\n",
    "dict_ = corpora.Dictionary(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using the dictionary\n",
    "doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.010*\"diokno\" + 0.009*\"inflation\" + 0.007*\"pbbm\" + 0.005*\"sir\" + 0.005*\"news\"'), (1, '0.011*\"philippine\" + 0.010*\"marcos\" + 0.010*\"world\" + 0.010*\"inflation\" + 0.009*\"economy\"'), (2, '0.018*\"k\" + 0.009*\"yung\" + 0.007*\"bbm\" + 0.006*\"inflation\" + 0.006*\"peso\"'), (3, '0.009*\"share\" + 0.006*\"mb\" + 0.006*\"price\" + 0.006*\"market\" + 0.006*\"company\"'), (4, '0.024*\"yung\" + 0.013*\"nyo\" + 0.010*\"bilihin\" + 0.010*\"wala\" + 0.007*\"mahal\"'), (5, '0.015*\"inflation\" + 0.008*\"philippine\" + 0.008*\"people\" + 0.008*\"dont\" + 0.007*\"country\"')]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to retrain LDAModel\n",
    "# %%script false\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus=doc_term_matrix, num_topics=6, id2word = dict_, passes=20, random_state=20, eval_every=None)\n",
    "\n",
    "# Prints the topics with the indexes: 0,1,2 :\n",
    "ldamodel.print_topics()\n",
    "# we need to manually check whethere the topics are different from one another or not\n",
    "print(ldamodel.print_topics(num_topics=6, num_words=5))\n",
    "\n",
    "# num_topics mean: how many topics want to extract\n",
    "# num_words: the number of words that want per topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment to prevent loading the pretrained model\n",
    "# %%script false\n",
    "\n",
    "import pickle\n",
    "\n",
    "ldamodel = None\n",
    "with open('./out/lda_model.pkl', 'rb') as f:\n",
    "    ldamodel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6006093464326839"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model = CoherenceModel(model=ldamodel, texts=clean_corpus, dictionary=dict_, coherence=\"c_v\")\n",
    "\n",
    "coherence_model.get_coherence() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.069121296141363\n",
      "64159.30412757534\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Gensim’s perplexity value is in logarithmic form. To compare with sklearn’s perplexity value np.exp(-1 *gensim.log_perplexity) is used\n",
    "\n",
    "print(ldamodel.log_perplexity(doc_term_matrix ))\n",
    "print(np.exp(-1 * ldamodel.log_perplexity(doc_term_matrix )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_rows = []\n",
    "\n",
    "for index,doc_in_words in enumerate(clean_corpus):\n",
    "    doc_in_words_as_string = ' '.join(doc_in_words)\n",
    "    row_vals = [dataframe.loc[index,'platform'], doc_in_words_as_string]+list(np.zeros(6))\n",
    "    doc_topics = ldamodel.get_document_topics(dict_.doc2bow(doc_in_words))\n",
    "    for doc_topic in doc_topics:\n",
    "        row_vals[doc_topic[0]+2] = doc_topic[1]\n",
    "    index_of_best_topic = np.argmax(row_vals[2:])\n",
    "    row_vals.append(index_of_best_topic+1)\n",
    "    all_rows.append(row_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Best Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>nowadays every thing seem increasing governanc...</td>\n",
       "      <td>0.872068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>disagree high</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.056123</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.720923</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.055822</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>wag tayong magalala naniniwala isusuprise sir ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577187</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>ok yang nang bansa ganyan selfish fanatic blen...</td>\n",
       "      <td>0.131092</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.683483</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>samasama tayong babaon</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.041702</td>\n",
       "      <td>0.041996</td>\n",
       "      <td>0.291638</td>\n",
       "      <td>0.540676</td>\n",
       "      <td>0.042255</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4720</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>mukhang nakashabu</td>\n",
       "      <td>0.055895</td>\n",
       "      <td>0.055894</td>\n",
       "      <td>0.383666</td>\n",
       "      <td>0.055898</td>\n",
       "      <td>0.392747</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>bbm mixed confusing economic term</td>\n",
       "      <td>0.347652</td>\n",
       "      <td>0.027958</td>\n",
       "      <td>0.027897</td>\n",
       "      <td>0.299905</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>0.268703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>mrutal utal</td>\n",
       "      <td>0.055670</td>\n",
       "      <td>0.055670</td>\n",
       "      <td>0.055671</td>\n",
       "      <td>0.388296</td>\n",
       "      <td>0.055672</td>\n",
       "      <td>0.389022</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>leni lutang</td>\n",
       "      <td>0.718660</td>\n",
       "      <td>0.056020</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>0.056879</td>\n",
       "      <td>0.056089</td>\n",
       "      <td>0.055592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>knowns wag puro kuda wala maitutulong magtraba...</td>\n",
       "      <td>0.089159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119481</td>\n",
       "      <td>0.391744</td>\n",
       "      <td>0.043806</td>\n",
       "      <td>0.348287</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4725 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Platform                                               Text   Topic 1  \\\n",
       "0     Facebook  nowadays every thing seem increasing governanc...  0.872068   \n",
       "1     Facebook                                      disagree high  0.056015   \n",
       "2     Facebook  wag tayong magalala naniniwala isusuprise sir ...  0.000000   \n",
       "3     Facebook  ok yang nang bansa ganyan selfish fanatic blen...  0.131092   \n",
       "4     Facebook                             samasama tayong babaon  0.041733   \n",
       "...        ...                                                ...       ...   \n",
       "4720   Youtube                                  mukhang nakashabu  0.055895   \n",
       "4721   Youtube                  bbm mixed confusing economic term  0.347652   \n",
       "4722   Youtube                                        mrutal utal  0.055670   \n",
       "4723   Youtube                                        leni lutang  0.718660   \n",
       "4724   Youtube  knowns wag puro kuda wala maitutulong magtraba...  0.089159   \n",
       "\n",
       "       Topic 2   Topic 3   Topic 4   Topic 5   Topic 6  Best Topic  \n",
       "0     0.000000  0.000000  0.090705  0.000000  0.000000           1  \n",
       "1     0.056123  0.055559  0.720923  0.055559  0.055822           4  \n",
       "2     0.000000  0.000000  0.383420  0.000000  0.577187           6  \n",
       "3     0.018532  0.018661  0.683483  0.018532  0.129700           4  \n",
       "4     0.041702  0.041996  0.291638  0.540676  0.042255           5  \n",
       "...        ...       ...       ...       ...       ...         ...  \n",
       "4720  0.055894  0.383666  0.055898  0.392747  0.055900           5  \n",
       "4721  0.027958  0.027897  0.299905  0.027884  0.268703           1  \n",
       "4722  0.055670  0.055671  0.388296  0.055672  0.389022           6  \n",
       "4723  0.056020  0.056760  0.056879  0.056089  0.055592           1  \n",
       "4724  0.000000  0.119481  0.391744  0.043806  0.348287           4  \n",
       "\n",
       "[4725 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "labelled_dataset = pd.DataFrame(all_rows, columns=[\"Platform\",\"Text\"]+[f\"Topic {i+1}\" for i in range(6)]+[\"Best Topic\"])\n",
    "labelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset.to_csv(\"./data/labelled_dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump model\n",
    "pickle.dump(ldamodel, open('./out/lda_model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
