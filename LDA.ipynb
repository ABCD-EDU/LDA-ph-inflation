{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "# for text preprocessing\n",
    "import re\n",
    "# import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "dataframe = pd.read_csv(\"./data/preprocessed.csv\")\n",
    "# clean_corpus = dataframe[\"preprocessed\"]\n",
    "# clean_corpus = [x.strip('][').split(', ') for x in dataframe[\"preprocessed\"]]\n",
    "# string to list\n",
    "clean_corpus = [ast.literal_eval(x) for x in dataframe[\"preprocessed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where every unique term is assigned an index.\n",
    "dict_ = corpora.Dictionary(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using the dictionary\n",
    "doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.016*\"inflation\" + 0.010*\"people\" + 0.010*\"philippine\" + 0.010*\"country\" + 0.008*\"dont\"'), (1, '0.009*\"share\" + 0.007*\"price\" + 0.006*\"mb\" + 0.006*\"market\" + 0.006*\"company\"'), (2, '0.027*\"yung\" + 0.007*\"leni\" + 0.007*\"bilihin\" + 0.007*\"inflation\" + 0.007*\"bansa\"'), (3, '0.017*\"nyo\" + 0.010*\"bbm\" + 0.008*\"wala\" + 0.008*\"inflation\" + 0.007*\"golden\"'), (4, '0.010*\"profit\" + 0.007*\"land\" + 0.006*\"build\" + 0.006*\"million\" + 0.006*\"q2\"'), (5, '0.017*\"unity\" + 0.007*\"question\" + 0.006*\"loan\" + 0.005*\"bbm\" + 0.005*\"marcos\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus=doc_term_matrix, num_topics=6, id2word = dict_, passes=20, random_state=20, eval_every=None)\n",
    "\n",
    "# Prints the topics with the indexes: 0,1,2 :\n",
    "ldamodel.print_topics()\n",
    "# we need to manually check whethere the topics are different from one another or not\n",
    "print(ldamodel.print_topics(num_topics=6, num_words=5))\n",
    "\n",
    "# num_topics mean: how many topics want to extract\n",
    "# num_words: the number of words that want per topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ldamodel, open('lda_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model = CoherenceModel(model=ldamodel, texts=clean_corpus, dictionary=dict_, coherence=\"c_v\")\n",
    "\n",
    "coherence_model.get_coherence() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.549481118559168\n",
      "5163.983614221804\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Gensim’s perplexity value is in logarithmic form. To compare with sklearn’s perplexity value np.exp(-1 *gensim.log_perplexity) is used\n",
    "\n",
    "print(ldamodel.log_perplexity(doc_term_matrix ))\n",
    "print(np.exp(-1 * ldamodel.log_perplexity(doc_term_matrix )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Facebook', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Reddit', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Tiktok', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Youtube', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Youtube', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Youtube', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Youtube', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Youtube', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Youtube', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n",
      "['Youtube', '', 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_rows = []\n",
    "\n",
    "for index,doc_in_words in enumerate(clean_corpus):\n",
    "    doc_in_words_as_string = ' '.join(doc_in_words)\n",
    "    row_vals = [dataframe.loc[index,'platform'], doc_in_words_as_string]+list(np.zeros(6))\n",
    "    doc_topics = ldamodel.get_document_topics(dict_.doc2bow(doc_in_words))\n",
    "    for doc_topic in doc_topics:\n",
    "        row_vals[doc_topic[0]+2] = doc_topic[1]\n",
    "    index_of_best_topic = np.argmax(row_vals[2:])\n",
    "    row_vals.append(index_of_best_topic+1)\n",
    "    all_rows.append(row_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Best Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>nowadays every thing seem increasing governanc...</td>\n",
       "      <td>0.872067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>disagree high</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.056150</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.720895</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.055822</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>wag tayong magalala naniniwala isusuprise sir ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577221</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>ok yang nang bansa ganyan selfish fanatic blen...</td>\n",
       "      <td>0.131091</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.018657</td>\n",
       "      <td>0.683489</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>samasama tayong babaon</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.041702</td>\n",
       "      <td>0.041996</td>\n",
       "      <td>0.291639</td>\n",
       "      <td>0.540683</td>\n",
       "      <td>0.042247</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>mukhang nakashabu</td>\n",
       "      <td>0.055581</td>\n",
       "      <td>0.055581</td>\n",
       "      <td>0.055694</td>\n",
       "      <td>0.055581</td>\n",
       "      <td>0.055581</td>\n",
       "      <td>0.721982</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>bbm mixed confusing economic term</td>\n",
       "      <td>0.542274</td>\n",
       "      <td>0.027945</td>\n",
       "      <td>0.027817</td>\n",
       "      <td>0.346267</td>\n",
       "      <td>0.027795</td>\n",
       "      <td>0.027902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>mrutal utal</td>\n",
       "      <td>0.055595</td>\n",
       "      <td>0.055595</td>\n",
       "      <td>0.055595</td>\n",
       "      <td>0.055595</td>\n",
       "      <td>0.722024</td>\n",
       "      <td>0.055596</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>leni lutang</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>0.309242</td>\n",
       "      <td>0.468519</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>knowns wag puro kuda wala maitutulong magtraba...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201202</td>\n",
       "      <td>0.684214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092754</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5209 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Platform                                               Text   Topic 1  \\\n",
       "0     Facebook  nowadays every thing seem increasing governanc...  0.872067   \n",
       "1     Facebook                                      disagree high  0.056015   \n",
       "2     Facebook  wag tayong magalala naniniwala isusuprise sir ...  0.000000   \n",
       "3     Facebook  ok yang nang bansa ganyan selfish fanatic blen...  0.131091   \n",
       "4     Facebook                             samasama tayong babaon  0.041733   \n",
       "...        ...                                                ...       ...   \n",
       "5204   Youtube                                  mukhang nakashabu  0.055581   \n",
       "5205   Youtube                  bbm mixed confusing economic term  0.542274   \n",
       "5206   Youtube                                        mrutal utal  0.055595   \n",
       "5207   Youtube                                        leni lutang  0.055560   \n",
       "5208   Youtube  knowns wag puro kuda wala maitutulong magtraba...  0.000000   \n",
       "\n",
       "       Topic 2   Topic 3   Topic 4   Topic 5   Topic 6  Best Topic  \n",
       "0     0.000000  0.000000  0.090707  0.000000  0.000000           1  \n",
       "1     0.056150  0.055559  0.720895  0.055559  0.055822           4  \n",
       "2     0.000000  0.000000  0.383386  0.000000  0.577221           6  \n",
       "3     0.018532  0.018657  0.683489  0.018532  0.129700           4  \n",
       "4     0.041702  0.041996  0.291639  0.540683  0.042247           5  \n",
       "...        ...       ...       ...       ...       ...         ...  \n",
       "5204  0.055581  0.055694  0.055581  0.055581  0.721982           6  \n",
       "5205  0.027945  0.027817  0.346267  0.027795  0.027902           1  \n",
       "5206  0.055595  0.055595  0.055595  0.722024  0.055596           5  \n",
       "5207  0.055560  0.309242  0.468519  0.055560  0.055560           4  \n",
       "5208  0.000000  0.201202  0.684214  0.000000  0.092754           4  \n",
       "\n",
       "[5209 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "labelled_dataset = pd.DataFrame(all_rows, columns=[\"Platform\",\"Text\"]+[f\"Topic {i+1}\" for i in range(6)]+[\"Best Topic\"])\n",
    "labelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset.to_csv(\"./data/labelled_dataset.csv\",index=False\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d5d9dcc67efa92ef875b9aa025d58eddcbf3c78cff45ba05987629fa8b55ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
