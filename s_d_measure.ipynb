{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import operator\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Best Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Why nowadays every thing  seem to be increasin...</td>\n",
       "      <td>nowadays every thing seem increasing governanc...</td>\n",
       "      <td>0.473528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124946</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>I will have to disagree.. we’re not that high!!</td>\n",
       "      <td>disagree high</td>\n",
       "      <td>0.056182</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.720307</td>\n",
       "      <td>0.055583</td>\n",
       "      <td>0.055932</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Wag po tayong mag-alala. Naniniwala po ako, is...</td>\n",
       "      <td>tayong magalala naniniwala isusuprise sir bbm ...</td>\n",
       "      <td>0.010470</td>\n",
       "      <td>0.139007</td>\n",
       "      <td>0.314224</td>\n",
       "      <td>0.247254</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.278587</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Ok lang yang lahat naman nang bansa ganyan. Sa...</td>\n",
       "      <td>ok yang nang bansa ganyan selfish fanatic blen...</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.018563</td>\n",
       "      <td>0.018567</td>\n",
       "      <td>0.618591</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.307228</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Sama-sama tayong BABAON muli.</td>\n",
       "      <td>samasama tayong babaon</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.041688</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.791544</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.041706</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Platform                                                Raw  \\\n",
       "0  Facebook  Why nowadays every thing  seem to be increasin...   \n",
       "1  Facebook    I will have to disagree.. we’re not that high!!   \n",
       "2  Facebook  Wag po tayong mag-alala. Naniniwala po ako, is...   \n",
       "3  Facebook  Ok lang yang lahat naman nang bansa ganyan. Sa...   \n",
       "4  Facebook                      Sama-sama tayong BABAON muli.   \n",
       "\n",
       "                                                Text   Topic 1   Topic 2  \\\n",
       "0  nowadays every thing seem increasing governanc...  0.473528  0.000000   \n",
       "1                                      disagree high  0.056182  0.055559   \n",
       "2  tayong magalala naniniwala isusuprise sir bbm ...  0.010470  0.139007   \n",
       "3  ok yang nang bansa ganyan selfish fanatic blen...  0.018524  0.018563   \n",
       "4                             samasama tayong babaon  0.041687  0.041688   \n",
       "\n",
       "    Topic 3   Topic 4   Topic 5   Topic 6  Best Topic  \n",
       "0  0.373507  0.000000  0.000000  0.124946           1  \n",
       "1  0.056438  0.720307  0.055583  0.055932           4  \n",
       "2  0.314224  0.247254  0.010458  0.278587           3  \n",
       "3  0.018567  0.618591  0.018527  0.307228           4  \n",
       "4  0.041687  0.791544  0.041687  0.041706           4  "
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('./data/labelled_dataset.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(dataframe[\"Text\"])\n",
    "raw_corpus = list(dataframe[\"Raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import pprint\n",
    "# counts = dict(Counter(corpus))\n",
    "# duplicates = {key:value for key, value in counts.items() if value > 1}\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_col_names = [f\"{i}\" for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_scores(matrix:list, type:str)->dict:\n",
    "   \n",
    "   pair_scores = {}\n",
    "   res = []\n",
    "   if type=='similarity':\n",
    "      res = matrix.idxmax(axis='columns')\n",
    "   else:\n",
    "      res = matrix.idxmin(axis='columns')\n",
    "      \n",
    "   for index, column in enumerate(res):\n",
    "      \n",
    "      test = matrix.loc[matrix.index[index], column]\n",
    "      \n",
    "      if  test < .5 and type==\"similarity\":\n",
    "         continue\n",
    "      elif test>=1:\n",
    "         continue\n",
    "         \n",
    "      doc1_len = len(raw_corpus[index].split(' ')) \n",
    "      doc2_len = len(raw_corpus[int(column)].split(' '))\n",
    "      # Exclude documents that are less than 4 and more than 30 characters\n",
    "      if doc1_len<=6 or doc2_len <=6 or doc1_len>=30 or doc2_len >=30:\n",
    "         continue\n",
    "      index = matrix.index[index]\n",
    "      if f\"{column}:{index}\" not in pair_scores.keys():\n",
    "         pair_scores[f\"{index}:{column}\"] = test\n",
    "   \n",
    "   return pair_scores\n",
    "\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    " \n",
    "def show_sentence_pairs(pairs:list):\n",
    "   for pair in pairs:\n",
    "      doc_pair_index = pair[0].split(':')\n",
    "      doc1_idx = int(doc_pair_index[0])\n",
    "      doc2_idx = int(doc_pair_index[1])\n",
    "\n",
    "      print(f\"Sentence 1: {raw_corpus[doc1_idx]} \\n Sentence 2: {raw_corpus[doc2_idx]}\")\n",
    "      print(f\"Similarity Score:{pair[1]}\")\n",
    "      print('-----------------------------------------------------------------')\n",
    "\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = dataframe['Best Topic'].nunique()\n",
    "topic_index_dict = {}\n",
    "def get_indices_of_doc_with_similar_topic():\n",
    "   dict_var = {}\n",
    "   for i in range(n_topics):\n",
    "      dict_var[i]=dataframe.index[dataframe['Best Topic']==i+1].tolist()\n",
    "   return dict_var\n",
    "topic_index_dict = get_indices_of_doc_with_similar_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tf_model = tfidf.fit(corpus)\n",
    "tfidf_matrix = tf_model.transform(corpus)\n",
    "\n",
    "# Generate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4686</th>\n",
       "      <th>4687</th>\n",
       "      <th>4688</th>\n",
       "      <th>4689</th>\n",
       "      <th>4690</th>\n",
       "      <th>4691</th>\n",
       "      <th>4692</th>\n",
       "      <th>4693</th>\n",
       "      <th>4694</th>\n",
       "      <th>4695</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1         2         3         4    5         6    7         8  \\\n",
       "0       NaN  0.0  0.000000  0.127159  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "1  0.000000  NaN  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.330497   \n",
       "2  0.000000  0.0       NaN  0.000000  0.097896  0.0  0.038161  0.0  0.000000   \n",
       "3  0.127159  0.0  0.000000       NaN  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "4  0.000000  0.0  0.097896  0.000000       NaN  0.0  0.000000  0.0  0.000000   \n",
       "\n",
       "     9  ...  4686  4687  4688  4689      4690  4691      4692  4693  4694  \\\n",
       "0  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "1  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "2  0.0  ...   0.0   0.0   0.0   0.0  0.019566   0.0  0.047418   0.0   0.0   \n",
       "3  0.0  ...   0.0   0.0   0.0   0.0  0.150336   0.0  0.000000   0.0   0.0   \n",
       "4  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "\n",
       "   4695  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 4696 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = pd.DataFrame(cosine_sim, index=row_col_names, columns=row_col_names )\n",
    "for i in range(len(similarity_matrix)): \n",
    "    similarity_matrix.iat[i, i] = np.nan\n",
    "similarity_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_similarity_df_by_topics = []\n",
    "def get_similarity_per_topics():\n",
    "   out =  []\n",
    "   for i in range(n_topics):\n",
    "      df1 = similarity_matrix.iloc[:,list(topic_index_dict[i])]\n",
    "      topicN_df = df1.iloc[list(topic_index_dict[i])]\n",
    "      out.append(topicN_df)\n",
    "   return out\n",
    "\n",
    "list_of_similarity_df_by_topics = get_similarity_per_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(list(topicN_df.iloc[:,0]))):\n",
    "#    if list(topicN_df.iloc[:,0])[i] != list(topicN_df.iloc[:,0])[i]:\n",
    "#       print(list(topicN_df.iloc[:,0])[i], list(topicN_df.iloc[:,0])[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity Score for Topic 1: 0.011550078742752259\n",
      "Average Similarity Score for Topic 2: 0.005862678270050148\n",
      "Average Similarity Score for Topic 3: 0.008277752102565747\n",
      "Average Similarity Score for Topic 4: 0.014220825364119692\n",
      "Average Similarity Score for Topic 5: 0.002798765632599675\n",
      "Average Similarity Score for Topic 6: 0.005319776602263192\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_topics):\n",
    "   print(f'Average Similarity Score for Topic {i+1}: {list_of_similarity_df_by_topics[i].iloc[:,0].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 | Documents 881\n",
      "Sentence 1: Perception is real. Truth is not. \n",
      " Sentence 2: Perception is real, truth is not for 6 yrs ba >_<\n",
      "Similarity Score:0.8668521944523396\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Enjoy the Vacation! \n",
      " Sentence 2: Enjoy your vacation, see you September 1\n",
      "Similarity Score:0.7863419979890762\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Thank you for educating us po.❤️ \n",
      " Sentence 2: Thank you for educating. Very clear explaination and on point po.\n",
      "Similarity Score:0.6619169066255127\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: convert your money in dollars \n",
      " Sentence 2: Convert your savings into the hardest money.\n",
      "Similarity Score:0.5536153603698053\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: More interest rate hikes please \n",
      " Sentence 2: If what you are saying is true that our inflation rate is different from other countries, then why do you need to hike interest rates?\n",
      "Similarity Score:0.5501181918446559\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 2 | Documents 647\n",
      "Sentence 1: accelerate talaga, parang f1 car ba yan sa bilis \n",
      " Sentence 2: Ang Bilis ng pag accelerate ng inflation parang F1 lng\n",
      "Similarity Score:0.8940873842304269\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: tsk tsk tsk \n",
      " Sentence 2: Boysit   750  tsk tsk\n",
      "Similarity Score:0.8681477299902255\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Build, Build, Build pa more \n",
      " Sentence 2: kala ko ba build build build? bakit bumagal?\n",
      "Similarity Score:0.847107834584554\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: sige anak pa ng anak \n",
      " Sentence 2: Mas better wag mag anak ng marami\n",
      "Similarity Score:0.6990628200544325\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Grabe tumaas ang bilis bilis, parang f1 car \n",
      " Sentence 2: accelerate talaga, parang f1 car ba yan sa bilis\n",
      "Similarity Score:0.6854447605463311\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 3 | Documents 994\n",
      "Sentence 1: I think I will have to disagree with that number 🥴 \n",
      " Sentence 2: I think that I will have to… I will have to disagree with that number. We are not that high. - 88M\n",
      "Similarity Score:0.8965798893896871\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: i think that number is not that high... \n",
      " Sentence 2: I think that I will have to… I will have to disagree with that number. We are not that high. - 88M\n",
      "Similarity Score:0.8197639753304548\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: High na high nga eh! \n",
      " Sentence 2: It’s not that high\n",
      "\n",
      "- a high person\n",
      "Similarity Score:0.8177974347642091\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: I think I will have to disagree... \n",
      " Sentence 2: I think I will have to disagree with that number 🥴\n",
      "Similarity Score:0.8161323832374467\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: thank you for explaining \n",
      " Sentence 2: thank you Sir for explaining very well ☺️\n",
      "Similarity Score:0.779713696408659\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 4 | Documents 994\n",
      "Sentence 1: I will have to disagree with that number. It's not that high. Unity lang bababa din yan. \n",
      " Sentence 2: I have to disagree with that number we are not that high.\n",
      "Similarity Score:0.8884986794826608\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Unity sagot dyan. \n",
      " Sentence 2: Unity at global lang ang sagot dyan\n",
      "Similarity Score:0.8791228375976882\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Pati inflation rate gustong i-revise \n",
      " Sentence 2: Hindi lang history ang gustong i-revise, pati pala inflation rate.\n",
      "Similarity Score:0.8409116229398275\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Kahit dito nagtaas na din mga bilihin.. \n",
      " Sentence 2: D2 rin s UAE nagtaas na rin Ang bilihin\n",
      "Similarity Score:0.7456654769408736\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Tatak Duterte! \n",
      " Sentence 2: Unity for Poverty!!! Tatak Marcos at Duterte.\n",
      "Similarity Score:0.7257470539393385\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 5 | Documents 351\n",
      "Sentence 1: > I reject your reality, and substitute my own. \n",
      " Sentence 2: So his admin's motto will be \"I reject your reality and substitute it with my trolls.\"\n",
      "Similarity Score:0.6780286069323442\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 6 | Documents 829\n",
      "Sentence 1: Fake news yan \n",
      " Sentence 2: Fake news ba to? 6.1%? We are not that high.\n",
      "Similarity Score:0.8762654322329054\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: 31M Pinipilit maging masaya \n",
      " Sentence 2: 14M lang naman yung unhappy. Yung 31M pinipilit naman nila maging masaya\n",
      "Similarity Score:0.8519797900609509\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Salamat po PBBM \n",
      " Sentence 2: SALAMAT PBBM!!  ONLY IN PBBM ADMINISTRATION\n",
      "Similarity Score:0.8459452888224637\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: what unhappy, unity lang po. \n",
      " Sentence 2: Kulang kayo sa Unity.  Bakit kayo unhappy, tumulong na lang kayo\n",
      "Similarity Score:0.7199728378529499\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Chismis lang daw yan, lahat ng facts ngayon chismis na \n",
      " Sentence 2: Chismis lang ang mataas na inflation. 😌\n",
      "Similarity Score:0.7055377219710337\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_topics):\n",
    "   print(f'Topic {i+1} | Documents {list_of_similarity_df_by_topics[i].shape[0]}')\n",
    "   \n",
    "   pair_scores = get_pairs_scores(list_of_similarity_df_by_topics[i] , \"similarity\")\n",
    "   topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1), reverse=True))\n",
    "   topN_pairs = take(5, topPairs.items())\n",
    "   show_sentence_pairs(topN_pairs)\n",
    "   print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4686</th>\n",
       "      <th>4687</th>\n",
       "      <th>4688</th>\n",
       "      <th>4689</th>\n",
       "      <th>4690</th>\n",
       "      <th>4691</th>\n",
       "      <th>4692</th>\n",
       "      <th>4693</th>\n",
       "      <th>4694</th>\n",
       "      <th>4695</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.669503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.849664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1         2         3         4    5         6    7         8  \\\n",
       "0       NaN  1.0  1.000000  0.872841  1.000000  1.0  1.000000  1.0  1.000000   \n",
       "1  1.000000  NaN  1.000000  1.000000  1.000000  1.0  1.000000  1.0  0.669503   \n",
       "2  1.000000  1.0       NaN  1.000000  0.902104  1.0  0.961839  1.0  1.000000   \n",
       "3  0.872841  1.0  1.000000       NaN  1.000000  1.0  1.000000  1.0  1.000000   \n",
       "4  1.000000  1.0  0.902104  1.000000       NaN  1.0  1.000000  1.0  1.000000   \n",
       "\n",
       "     9  ...  4686  4687  4688  4689      4690  4691      4692  4693  4694  \\\n",
       "0  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "1  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "2  1.0  ...   1.0   1.0   1.0   1.0  0.980434   1.0  0.952582   1.0   1.0   \n",
       "3  1.0  ...   1.0   1.0   1.0   1.0  0.849664   1.0  1.000000   1.0   1.0   \n",
       "4  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "\n",
       "   4695  \n",
       "0   1.0  \n",
       "1   1.0  \n",
       "2   1.0  \n",
       "3   1.0  \n",
       "4   1.0  \n",
       "\n",
       "[5 rows x 4696 columns]"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate cosine similarity\n",
    "cosine_distance = cosine_distances(tfidf_matrix, tfidf_matrix)\n",
    "distance_matrix = pd.DataFrame(cosine_distance, index=row_col_names, columns=row_col_names )\n",
    "for i in range(len(distance_matrix)): \n",
    "    distance_matrix.iat[i, i] = np.nan\n",
    "distance_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res= topicN_df.idxmax(axis='columns')\n",
    "# print(res)\n",
    "# maxes = []\n",
    "# test_shit = []\n",
    "# for index, column in enumerate(res):\n",
    "#    test_shit.append(int(column))\n",
    "#    test = topicN_df.loc[topicN_df.index[index], column]\n",
    "#    maxes.append(test)\n",
    "# print(mean(maxes))\n",
    "# print(len(maxes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pair_scores = get_pairs_scores(distance_matrix, \"distance\")\n",
    "# topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1),reverse=True))\n",
    "# topN_pairs = take(20, topPairs.items())\n",
    "# show_sentence_pairs(topN_pairs)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d5d9dcc67efa92ef875b9aa025d58eddcbf3c78cff45ba05987629fa8b55ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
