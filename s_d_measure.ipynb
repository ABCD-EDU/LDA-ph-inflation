{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import operator\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Best Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Why nowadays every thing  seem to be increasin...</td>\n",
       "      <td>nowadays every thing seem increasing governanc...</td>\n",
       "      <td>0.473528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124946</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>I will have to disagree.. we‚Äôre not that high!!</td>\n",
       "      <td>disagree high</td>\n",
       "      <td>0.056182</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.056438</td>\n",
       "      <td>0.720307</td>\n",
       "      <td>0.055583</td>\n",
       "      <td>0.055932</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Wag po tayong mag-alala. Naniniwala po ako, is...</td>\n",
       "      <td>tayong magalala naniniwala isusuprise sir bbm ...</td>\n",
       "      <td>0.010470</td>\n",
       "      <td>0.139007</td>\n",
       "      <td>0.314224</td>\n",
       "      <td>0.247254</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.278587</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Ok lang yang lahat naman nang bansa ganyan. Sa...</td>\n",
       "      <td>ok yang nang bansa ganyan selfish fanatic blen...</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.018563</td>\n",
       "      <td>0.018567</td>\n",
       "      <td>0.618591</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.307228</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Sama-sama tayong BABAON muli.</td>\n",
       "      <td>samasama tayong babaon</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.041688</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.791544</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.041706</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Platform                                                Raw  \\\n",
       "0  Facebook  Why nowadays every thing  seem to be increasin...   \n",
       "1  Facebook    I will have to disagree.. we‚Äôre not that high!!   \n",
       "2  Facebook  Wag po tayong mag-alala. Naniniwala po ako, is...   \n",
       "3  Facebook  Ok lang yang lahat naman nang bansa ganyan. Sa...   \n",
       "4  Facebook                      Sama-sama tayong BABAON muli.   \n",
       "\n",
       "                                                Text   Topic 1   Topic 2  \\\n",
       "0  nowadays every thing seem increasing governanc...  0.473528  0.000000   \n",
       "1                                      disagree high  0.056182  0.055559   \n",
       "2  tayong magalala naniniwala isusuprise sir bbm ...  0.010470  0.139007   \n",
       "3  ok yang nang bansa ganyan selfish fanatic blen...  0.018524  0.018563   \n",
       "4                             samasama tayong babaon  0.041687  0.041688   \n",
       "\n",
       "    Topic 3   Topic 4   Topic 5   Topic 6  Best Topic  \n",
       "0  0.373507  0.000000  0.000000  0.124946           1  \n",
       "1  0.056438  0.720307  0.055583  0.055932           4  \n",
       "2  0.314224  0.247254  0.010458  0.278587           3  \n",
       "3  0.018567  0.618591  0.018527  0.307228           4  \n",
       "4  0.041687  0.791544  0.041687  0.041706           4  "
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('./data/labelled_dataset.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(dataframe[\"Text\"])\n",
    "raw_corpus = list(dataframe[\"Raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import pprint\n",
    "# counts = dict(Counter(corpus))\n",
    "# duplicates = {key:value for key, value in counts.items() if value > 1}\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_col_names = [f\"{i}\" for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_scores(matrix:list, type:str)->dict:\n",
    "   \n",
    "   pair_scores = {}\n",
    "   res = []\n",
    "   if type=='similarity':\n",
    "      res = matrix.idxmax(axis='columns')\n",
    "   else:\n",
    "      res = matrix.idxmin(axis='columns')\n",
    "      \n",
    "   for index, column in enumerate(res):\n",
    "      \n",
    "      test = matrix.loc[matrix.index[index], column]\n",
    "      \n",
    "      if  test < .5 and test>=1:\n",
    "         continue\n",
    "         \n",
    "      doc1_len = len(corpus[index].split(' ')) \n",
    "      doc2_len = len(corpus[int(column)].split(' '))\n",
    "      # Exclude documents that are less than 4 and more than 30 characters\n",
    "      if doc1_len<=3 or doc2_len <=3 or doc1_len>=8 or doc2_len >=8:\n",
    "         continue\n",
    "      index = matrix.index[index]\n",
    "      if f\"{column}:{index}\" not in pair_scores.keys():\n",
    "         pair_scores[f\"{index}:{column}\"] = test\n",
    "   \n",
    "   return pair_scores\n",
    "\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    " \n",
    "def show_sentence_pairs(pairs:list):\n",
    "   for pair in pairs:\n",
    "      doc_pair_index = pair[0].split(':')\n",
    "      doc1_idx = int(doc_pair_index[0])\n",
    "      doc2_idx = int(doc_pair_index[1])\n",
    "\n",
    "      print(f\"Sentence 1: {raw_corpus[doc1_idx]} \\n Sentence 2: {raw_corpus[doc2_idx]}\")\n",
    "      if type == \"similarity\":\n",
    "         print(f\"Similarity Score:{pair[1]}\")\n",
    "      else:\n",
    "         print(f\"Dissimilarity Score:{pair[1]}\")\n",
    "      print('-----------------------------------------------------------------')\n",
    "\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = dataframe['Best Topic'].nunique()\n",
    "topic_index_dict = {}\n",
    "def get_indices_of_doc_with_similar_topic():\n",
    "   dict_var = {}\n",
    "   for i in range(n_topics):\n",
    "      dict_var[i]=dataframe.index[dataframe['Best Topic']==i+1].tolist()\n",
    "   return dict_var\n",
    "topic_index_dict = get_indices_of_doc_with_similar_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tf_model = tfidf.fit(corpus)\n",
    "tfidf_matrix = tf_model.transform(corpus)\n",
    "\n",
    "# Generate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4686</th>\n",
       "      <th>4687</th>\n",
       "      <th>4688</th>\n",
       "      <th>4689</th>\n",
       "      <th>4690</th>\n",
       "      <th>4691</th>\n",
       "      <th>4692</th>\n",
       "      <th>4693</th>\n",
       "      <th>4694</th>\n",
       "      <th>4695</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 4696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1         2         3         4    5         6    7         8  \\\n",
       "0       NaN  0.0  0.000000  0.127159  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "1  0.000000  NaN  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.330497   \n",
       "2  0.000000  0.0       NaN  0.000000  0.097896  0.0  0.038161  0.0  0.000000   \n",
       "3  0.127159  0.0  0.000000       NaN  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "4  0.000000  0.0  0.097896  0.000000       NaN  0.0  0.000000  0.0  0.000000   \n",
       "\n",
       "     9  ...  4686  4687  4688  4689      4690  4691      4692  4693  4694  \\\n",
       "0  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "1  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "2  0.0  ...   0.0   0.0   0.0   0.0  0.019566   0.0  0.047418   0.0   0.0   \n",
       "3  0.0  ...   0.0   0.0   0.0   0.0  0.150336   0.0  0.000000   0.0   0.0   \n",
       "4  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "\n",
       "   4695  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 4696 columns]"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = pd.DataFrame(cosine_sim, index=row_col_names, columns=row_col_names )\n",
    "for i in range(len(similarity_matrix)): \n",
    "    similarity_matrix.iat[i, i] = np.nan\n",
    "similarity_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_similarity_df_by_topics = []\n",
    "def get_similarity_per_topics():\n",
    "   out =  []\n",
    "   for i in range(n_topics):\n",
    "      df1 = similarity_matrix.iloc[:,list(topic_index_dict[i])]\n",
    "      topicN_df = df1.iloc[list(topic_index_dict[i])]\n",
    "      out.append(topicN_df)\n",
    "   return out\n",
    "\n",
    "list_of_similarity_df_by_topics = get_similarity_per_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(list(topicN_df.iloc[:,0]))):\n",
    "#    if list(topicN_df.iloc[:,0])[i] != list(topicN_df.iloc[:,0])[i]:\n",
    "#       print(list(topicN_df.iloc[:,0])[i], list(topicN_df.iloc[:,0])[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity Score for Topic 1: 0.011550078742752259\n",
      "Average Similarity Score for Topic 2: 0.005862678270050148\n",
      "Average Similarity Score for Topic 3: 0.008277752102565747\n",
      "Average Similarity Score for Topic 4: 0.014220825364119692\n",
      "Average Similarity Score for Topic 5: 0.002798765632599675\n",
      "Average Similarity Score for Topic 6: 0.005319776602263192\n",
      "\n",
      "Topic 1 | Documents 881\n",
      "Sentence 1: sure honey, whatever helps you sleep at night ü•¥ \n",
      " Sentence 2: sure honey, whatever helps you sleep at night\n",
      "Dissimilarity Score:1.0000000000000002\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Perception is real. Truth is not. \n",
      " Sentence 2: Perception is real, truth is not for 6 yrs ba >_<\n",
      "Dissimilarity Score:0.8668521944523396\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Enjoy the Vacation! \n",
      " Sentence 2: Enjoy your vacation, see you September 1\n",
      "Dissimilarity Score:0.7863419979890762\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: CHANGE IS COMING! \n",
      " Sentence 2: don't worry change is coming\n",
      "Dissimilarity Score:0.7163456238305796\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Filipinos are unhappy about everything \n",
      " Sentence 2: How many unhappy Filipinos Daw 31M?\n",
      "Dissimilarity Score:0.5966338764804713\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 2 | Documents 647\n",
      "Sentence 1: Ang Bilis ng pag accelerate ng inflation parang F1 lng \n",
      " Sentence 2: accelerate talaga, parang f1 car ba yan sa bilis\n",
      "Dissimilarity Score:0.8940873842304269\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Build, Build, Build pa more \n",
      " Sentence 2: kala ko ba build build build? bakit bumagal?\n",
      "Dissimilarity Score:0.847107834584554\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Grabe tumaas ang bilis bilis, parang f1 car \n",
      " Sentence 2: accelerate talaga, parang f1 car ba yan sa bilis\n",
      "Dissimilarity Score:0.6854447605463311\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Thank you ABS CBN. \n",
      " Sentence 2: Bias KC Ang abs cbn\n",
      "Dissimilarity Score:0.6782765042246364\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Nagtatanim ka ba or bumibili ka lang ng bigas sa palengke? \n",
      " Sentence 2: sa shopwise or sa palengke kami bumibili ng bigas kaya alam namin üòè\n",
      "Dissimilarity Score:0.6130950756628741\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 3 | Documents 994\n",
      "Sentence 1: \"I think I will have to disagree with that number. We are not that high\" \n",
      " Sentence 2: I think that I will have to‚Ä¶ I will have to disagree with that number. We are not that high. - 88M\n",
      "Dissimilarity Score:1.0000000000000002\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: \"I think that I will have to disagree with that number. We are not that high.\" \n",
      " Sentence 2: I think that I will have to‚Ä¶ I will have to disagree with that number. We are not that high. - 88M\n",
      "Dissimilarity Score:1.0000000000000002\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: \"I disagree with that number I think we're not that high\" \n",
      " Sentence 2: I think that I will have to‚Ä¶ I will have to disagree with that number. We are not that high. - 88M\n",
      "Dissimilarity Score:1.0000000000000002\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: I think I will have to disagree with that number ü•¥ \n",
      " Sentence 2: I think that I will have to‚Ä¶ I will have to disagree with that number. We are not that high. - 88M\n",
      "Dissimilarity Score:0.8965798893896871\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Salamat Sir. \n",
      " Sentence 2: Sir sir sir papataasin po ba natin? Salamat po sa unity\n",
      "Dissimilarity Score:0.76714392600891\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 4 | Documents 994\n",
      "Sentence 1: Sabay sabay tayong lulubog muli \n",
      " Sentence 2: UNITY LANG. SABAY SABAY TAYONG LULUBOG\n",
      "Dissimilarity Score:0.9696405734569837\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: I have to disagree with that number we are not that high. \n",
      " Sentence 2: I will have to disagree with that number. It's not that high. Unity lang bababa din yan.\n",
      "Dissimilarity Score:0.8884986794826608\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Unity sagot dyan. \n",
      " Sentence 2: Unity at global lang ang sagot dyan\n",
      "Dissimilarity Score:0.8791228375976882\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: lahat naman tumataas kahit sa ibang bansa \n",
      " Sentence 2: Lahat naman apektado kahit nga sa ibang bansa lahat tumataas ang presyo ng bilihin.\n",
      "Dissimilarity Score:0.6984921595467217\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Buong mundo po yan di lang jan sa pinas. \n",
      " Sentence 2: Hindi lng po sa Pinas, ramdam po yan sa buong mundo,.\n",
      "Dissimilarity Score:0.6750578609628424\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 5 | Documents 351\n",
      "Sentence 1: > I reject your reality, and substitute my own. \n",
      " Sentence 2: So his admin's motto will be \"I reject your reality and substitute it with my trolls.\"\n",
      "Dissimilarity Score:0.6780286069323442\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: First 100 days \n",
      " Sentence 2: That‚Äôs just the first 100 days folks!\n",
      "Dissimilarity Score:0.6166393684510556\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: My kudos to Sec. Diokno \n",
      " Sentence 2: Kudos sir benjamin diokno!!! \n",
      "Dissimilarity Score:0.6070160771412343\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Surprise! Ang bagong meaning ng NGO is Non-Government Association, yesss slayyyy VP LeniüòçüíÖ \n",
      " Sentence 2: matagal nang Non-Government Association ang ibig sabihin ng NGO.\n",
      "Dissimilarity Score:0.4934970382638523\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: ROEL ABABA TARGET KALBO PHILIPPINE NATIONAL POLICE PNP CHIEF SENATOR RONALD BATO DELA ROSA \n",
      " Sentence 2: ROEL ABABA TARGET KALBO MICHAEL JORDAN\n",
      "Dissimilarity Score:0.44622018031705335\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 6 | Documents 829\n",
      "Sentence 1: Bagong Pilipinas \n",
      " Sentence 2: Bagong pilipinas bagong inflation hehehe0\n",
      "Dissimilarity Score:0.8591547634960202\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Salamat po PBBM \n",
      " Sentence 2: SALAMAT PBBM!!  ONLY IN PBBM ADMINISTRATION\n",
      "Dissimilarity Score:0.8459452888224637\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: \"Tuloy na tuloy pa rin \n",
      " Sentence 2: sige tuloy tuloy lang mga 6yrs\n",
      "Dissimilarity Score:0.8209765905973414\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Mga Well Educated kasi po sila ..üòÇüòÇüòÇüòÇüòÇüòÇüòÇ \n",
      " Sentence 2: ganyan tlga pag well educated\n",
      "Dissimilarity Score:0.6802279226939774\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1:  \"Di LaNg nAmAn tAyO aNg naKaKarAnaS nyAn\" ü§° \n",
      " Sentence 2: Hindi lang pilipinas ang nakakaranas ng inflation... buong mundo nakakaranas ng inflation...\n",
      "Dissimilarity Score:0.6047393165282646\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_topics):\n",
    "   print(f'Average Similarity Score for Topic {i+1}: {list_of_similarity_df_by_topics[i].iloc[:,0].mean()}')\n",
    "print('')\n",
    "for i in range(n_topics):\n",
    "   print(f'Topic {i+1} | Documents {list_of_similarity_df_by_topics[i].shape[0]}')\n",
    "   \n",
    "   pair_scores = get_pairs_scores(list_of_similarity_df_by_topics[i] , \"similarity\")\n",
    "   topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1), reverse=True))\n",
    "   topN_pairs = take(5, topPairs.items())\n",
    "   show_sentence_pairs(topN_pairs)\n",
    "   print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4686</th>\n",
       "      <th>4687</th>\n",
       "      <th>4688</th>\n",
       "      <th>4689</th>\n",
       "      <th>4690</th>\n",
       "      <th>4691</th>\n",
       "      <th>4692</th>\n",
       "      <th>4693</th>\n",
       "      <th>4694</th>\n",
       "      <th>4695</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.669503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.849664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 4696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1         2         3         4    5         6    7         8  \\\n",
       "0       NaN  1.0  1.000000  0.872841  1.000000  1.0  1.000000  1.0  1.000000   \n",
       "1  1.000000  NaN  1.000000  1.000000  1.000000  1.0  1.000000  1.0  0.669503   \n",
       "2  1.000000  1.0       NaN  1.000000  0.902104  1.0  0.961839  1.0  1.000000   \n",
       "3  0.872841  1.0  1.000000       NaN  1.000000  1.0  1.000000  1.0  1.000000   \n",
       "4  1.000000  1.0  0.902104  1.000000       NaN  1.0  1.000000  1.0  1.000000   \n",
       "\n",
       "     9  ...  4686  4687  4688  4689      4690  4691      4692  4693  4694  \\\n",
       "0  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "1  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "2  1.0  ...   1.0   1.0   1.0   1.0  0.980434   1.0  0.952582   1.0   1.0   \n",
       "3  1.0  ...   1.0   1.0   1.0   1.0  0.849664   1.0  1.000000   1.0   1.0   \n",
       "4  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "\n",
       "   4695  \n",
       "0   1.0  \n",
       "1   1.0  \n",
       "2   1.0  \n",
       "3   1.0  \n",
       "4   1.0  \n",
       "\n",
       "[5 rows x 4696 columns]"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate cosine similarity\n",
    "cosine_distance = cosine_distances(tfidf_matrix, tfidf_matrix)\n",
    "distance_matrix = pd.DataFrame(cosine_distance, index=row_col_names, columns=row_col_names )\n",
    "for i in range(len(distance_matrix)): \n",
    "    distance_matrix.iat[i, i] = np.nan\n",
    "distance_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dissimilarity_df_by_topics = []\n",
    "def get_dissimilarity_per_topics():\n",
    "   out =  []\n",
    "   for i in range(n_topics):\n",
    "      df1 = distance_matrix.iloc[:,list(topic_index_dict[i])]\n",
    "      topicN_df = df1.iloc[list(topic_index_dict[i])]\n",
    "      out.append(topicN_df)\n",
    "   return out\n",
    "\n",
    "list_of_dissimilarity_df_by_topics = get_dissimilarity_per_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dissimilarity Score for Topic 1: 0.9884499212572477\n",
      "Average Dissimilarity Score for Topic 2: 0.99413732172995\n",
      "Average Dissimilarity Score for Topic 3: 0.9917222478974342\n",
      "Average Dissimilarity Score for Topic 4: 0.9857791746358803\n",
      "Average Dissimilarity Score for Topic 5: 0.9972012343674003\n",
      "Average Dissimilarity Score for Topic 6: 0.9946802233977369\n",
      "\n",
      "Topic 1 | Documents 881\n",
      "Sentence 1: Sige lang guys ! Okay lang yan! Ipinanganak tayung sexbomb ! Kaya unity po ang sagut sa kahirapan HAHAHAH dasurv po natin si BBM na mag outing with friends \n",
      " Sentence 2: This is so good! Do you guys have an email subscription?\n",
      "Dissimilarity Score:0.9007963139106672\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: A lot of people are very quick to jumping a gun to quickly without understanding the full context of the statement. Para lang maka panira agad. \n",
      " Sentence 2: lahat nakatingin, kaya pag hindi nila naintindihan. issue agad.\n",
      "Dissimilarity Score:0.8984152956720199\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Usury or lending money with interest is forbidden and cursed by Allah ! \n",
      " Sentence 2: More interest rate hikes please\n",
      "Dissimilarity Score:0.8832883537790351\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: kaya mtagal pa katutupad ang 20 kilos na bigas gawa Ng agricultural illegal smuggling. \n",
      " Sentence 2: 240 kilo ng sibuyas sa amin, magkano sa inyo?\n",
      "Dissimilarity Score:0.8760817261134032\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: tomo lods,we should not wish ones gov'ts failure unlike the dilawan whose main goal is to destabilize a govt same as CIA...üò≥üò≥ \n",
      " Sentence 2: Their only goal was to win, not to serve the people!\n",
      "Dissimilarity Score:0.8726478006955722\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 2 | Documents 647\n",
      "Sentence 1: Some HMO options, OP: https://grit.ph/hmo/ \n",
      " Sentence 2: nagkakaisa lahat ng bilihin. magtaas ang isa magtaasan din ang lahat! yan ang tunay na unity!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: IYAKKK PAAAA \n",
      " Sentence 2: nagkakaisa lahat ng bilihin. magtaas ang isa magtaasan din ang lahat! yan ang tunay na unity!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: @eivlaelocin @itsmejk09 @mcpatootie \n",
      " Sentence 2: nagkakaisa lahat ng bilihin. magtaas ang isa magtaasan din ang lahat! yan ang tunay na unity!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: @knvbhzvb @parakangtangaewan \n",
      " Sentence 2: nagkakaisa lahat ng bilihin. magtaas ang isa magtaasan din ang lahat! yan ang tunay na unity!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Maaayos dn ang lht. \n",
      " Sentence 2: nagkakaisa lahat ng bilihin. magtaas ang isa magtaasan din ang lahat! yan ang tunay na unity!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 3 | Documents 994\n",
      "Sentence 1: normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "normal lang ang inflation\n",
      "\n",
      "*rocks back and forth \n",
      " Sentence 2: His back must hurt for carrying Philippine's economy behind his back.\n",
      "Dissimilarity Score:0.9774862627106706\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Di maikakaila na nagmahal lahat. Kailangan na mag tough decisions sa supermarket parati to the point na stressful na mag grocery for real. di mo na pwedeng bilhin lahat ng gusto mo sa same budget at all. Seryosong mapapahanap ka na bigla ng mas murang alternatives/brands sa nakasanayan or kailangan mo na mag give up ng things na di importante (beer, chocolate, chichirya etc). Ang saklap taena \n",
      " Sentence 2: good explanation kuys dumami bigla ang economistüòÇüòÇüòÇ\n",
      "Dissimilarity Score:0.9122816959002665\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Our family is now planting our own food, kung di to applicable sa place nyo, try to grow kahit mga naka-paso na pwedeng indoor. Less meat na rin. Tapos ewan pero nakatulong siguro na di rin kami gumigising ng maaga kaya 2√ó a day lang kain namin ‚Äì brunch, then dinner at 6pm or 7pm.\n",
      "\n",
      "Mindful na rin sa consumption ng kuryente ang laki kasi ng kain non from our budget, pinag iisipan na rin namin kung paano kami makalilipat into using solar panels, para maalis na yung electric bill in the future. Sabi ko nga sa nanay ko kung pwedeng tumira na lang off grid eh hahaha! Para walang monthly bills kaso wala naman kaming sariling lupa to install things na gagamitin namin to sustain that lifestyle. \n",
      "\n",
      "Tapos we're also investing into our health, mahal magkasakit  eh so we take vitamins na dati naman hindi health insurance is yet to come once mapalaki pa namin income namin. \n",
      " Sentence 2: That's the unity of 31 M.,.100 days of non- sense.\n",
      "Dissimilarity Score:0.9067054309515479\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Gago maghanap kasi naman kayo ng matinong leader na kalaban.  Madali lang sana talunin si bbm eh.  Kaso tangin yung manok nyo yung parang tanga na humahadouken. Dapat si isko na lang sinupportahan nyo easy win pa. \n",
      " Sentence 2: Philippines collapsing under a bbm win... This is going to get real bad.\n",
      "Dissimilarity Score:0.8864460816900586\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Better situation than africa,  somalia etc \n",
      " Sentence 2: Omg,all over the world,all increase in prices,essential goods etc.\n",
      "Dissimilarity Score:0.8804598216468356\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 4 | Documents 994\n",
      "Sentence 1: if bababa ang presyo ng binhi at abono at kung ang goods ay deretso sa market possible poü•∞ \n",
      " Sentence 2: baba ang presyo ng bigas? sure ba tlga?\n",
      "Dissimilarity Score:0.8822877638829798\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Haba but worth digesting. Take my upvote! \n",
      " Sentence 2: Tpos ung iba nagkakagulo sila dun sa ticket na worth 15k\n",
      "Dissimilarity Score:0.8771370381068818\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Sorry but debunking claims and Fact-Checking doesnt work when convincing a radical person, according to research.\n",
      "\n",
      "https://fivethirtyeight.com/features/fact-checking-wont-save-us-from-fake-news/ \n",
      " Sentence 2: Sorry to say this... BOBO AMPUTA\n",
      "Dissimilarity Score:0.8745137637241474\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: disagrees on factual data bruh this yalls president? \n",
      " Sentence 2: sakit sa ulo basta data na eh üò≠üò≠üòµ\n",
      "Dissimilarity Score:0.8694633938277687\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Usok ba to sa isang civil war ng duterte at Marcos sa kapulisan? I think kayangkaya icover up ng pulis to if gugustuhin lng nila. Pero bakit lumabas? \n",
      " Sentence 2: Unity for Poverty!!! Tatak Marcos at Duterte.\n",
      "Dissimilarity Score:0.8567273394182245\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 5 | Documents 351\n",
      "Sentence 1: Malakasang Yuniteee \n",
      " Sentence 2: Nakakaproud! High numbers talaga ang naacchieve ng PBBM! Labyu po!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Sakto ng Sakto ung 31% \n",
      " Sentence 2: Nakakaproud! High numbers talaga ang naacchieve ng PBBM! Labyu po!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Ginusto yarn! Kaya yarn \n",
      " Sentence 2: Nakakaproud! High numbers talaga ang naacchieve ng PBBM! Labyu po!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Galawang [boy-dolomite](https://i.dailymail.co.uk/i/pix/2013/12/18/article-2525602-1A2B2A3600000578-553_634x408.jpg) talaga. \n",
      " Sentence 2: Nakakaproud! High numbers talaga ang naacchieve ng PBBM! Labyu po!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Sinira mo nga damit mo nagreklamo ba kami??üòÇüòÇüòÇ \n",
      " Sentence 2: Nakakaproud! High numbers talaga ang naacchieve ng PBBM! Labyu po!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 6 | Documents 829\n",
      "Sentence 1: Set your alarms tomorrow mga Simps HAHAHAHAHAHHA \n",
      " Sentence 2: Kasing bilis ng Formula1 race LOL!\n",
      "Dissimilarity Score:1.0\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Ok lng po ba kayo?Kasi kami hindi e,kaya magsipagtrabaho kayo!Ginusto nyo ung posisyon dba? \n",
      " Sentence 2: OK lang yan buong mundo naman yan nangyayari */s*\n",
      "Dissimilarity Score:0.8869929865635695\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: I feel ya. Ako naman licensed architect dito, same dilemma LOL. Pero karamihan ng kakilala ko na CE, abroad talaga naghahanap. Nagwowork lang dito sa Pinas for a couple of years pagkapasa ng boards tapos hanap na ng work abroad.\n",
      "\n",
      "Good luck sa board exams, OP! ‚ú®‚ú®‚ú® \n",
      " Sentence 2: bitter kasi talaga sila naghahanap lang yan sila ng maiibato kay PBBM.\n",
      "Dissimilarity Score:0.8815107637428683\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: HAHAHAHH coming from someone na nagpeke ng Oxford degree nya. haay nako. \n",
      " Sentence 2: Nag disagree ang walang degree lmao pick a struggle dugyot\n",
      "Dissimilarity Score:0.8697518478211825\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: People reacting like they know what‚Äôs happening. Pag nakita nila tumaas inflation its because of the president na magbabakasyon lng sa SG. Kaya nga tinawag na bias news kasi 1 sided lng pinapakita nila. Di nila pinakita ilang billion investment nkuha n‚Ä¶ \n",
      " Sentence 2: Tumaas ang rating ni Jr dahil tumaas daw ang inflation\n",
      "Dissimilarity Score:0.8686204352570057\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_topics):\n",
    "   print(f'Average Dissimilarity Score for Topic {i+1}: {list_of_dissimilarity_df_by_topics[i].iloc[:,0].mean()}')\n",
    "print('')\n",
    "for i in range(n_topics):\n",
    "   print(f'Topic {i+1} | Documents {list_of_dissimilarity_df_by_topics[i].shape[0]}')\n",
    "   \n",
    "   pair_scores = get_pairs_scores(list_of_dissimilarity_df_by_topics[i] , \"dissimilarity\")\n",
    "   topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1), reverse=True))\n",
    "   topN_pairs = take(5, topPairs.items())\n",
    "   show_sentence_pairs(topN_pairs)\n",
    "   print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res= topicN_df.idxmax(axis='columns')\n",
    "# print(res)\n",
    "# maxes = []\n",
    "# test_shit = []\n",
    "# for index, column in enumerate(res):\n",
    "#    test_shit.append(int(column))\n",
    "#    test = topicN_df.loc[topicN_df.index[index], column]\n",
    "#    maxes.append(test)\n",
    "# print(mean(maxes))\n",
    "# print(len(maxes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pair_scores = get_pairs_scores(distance_matrix, \"distance\")\n",
    "# topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1),reverse=True))\n",
    "# topN_pairs = take(20, topPairs.items())\n",
    "# show_sentence_pairs(topN_pairs)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d5d9dcc67efa92ef875b9aa025d58eddcbf3c78cff45ba05987629fa8b55ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
