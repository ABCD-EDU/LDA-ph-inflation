{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import operator\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Best Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Why nowadays every thing  seem to be increasin...</td>\n",
       "      <td>nowadays every thing seem increasing governanc...</td>\n",
       "      <td>0.960316</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>I will have to disagree.. we’re not that high!!</td>\n",
       "      <td>disagree high</td>\n",
       "      <td>0.772866</td>\n",
       "      <td>0.114139</td>\n",
       "      <td>0.112995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Wag po tayong mag-alala. Naniniwala po ako, is...</td>\n",
       "      <td>tayong magalala naniniwala isusuprise sir bbm ...</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.957550</td>\n",
       "      <td>0.020991</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Ok lang yang lahat naman nang bansa ganyan. Sa...</td>\n",
       "      <td>ok yang nang bansa ganyan selfish fanatic blen...</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>0.925430</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Sama-sama tayong BABAON muli.</td>\n",
       "      <td>samasama tayong babaon</td>\n",
       "      <td>0.083516</td>\n",
       "      <td>0.832983</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Platform                                                Raw  \\\n",
       "0  Facebook  Why nowadays every thing  seem to be increasin...   \n",
       "1  Facebook    I will have to disagree.. we’re not that high!!   \n",
       "2  Facebook  Wag po tayong mag-alala. Naniniwala po ako, is...   \n",
       "3  Facebook  Ok lang yang lahat naman nang bansa ganyan. Sa...   \n",
       "4  Facebook                      Sama-sama tayong BABAON muli.   \n",
       "\n",
       "                                                Text   Topic 1   Topic 2  \\\n",
       "0  nowadays every thing seem increasing governanc...  0.960316  0.019696   \n",
       "1                                      disagree high  0.772866  0.114139   \n",
       "2  tayong magalala naniniwala isusuprise sir bbm ...  0.021459  0.957550   \n",
       "3  ok yang nang bansa ganyan selfish fanatic blen...  0.037489  0.925430   \n",
       "4                             samasama tayong babaon  0.083516  0.832983   \n",
       "\n",
       "    Topic 3  Best Topic  \n",
       "0  0.019989           1  \n",
       "1  0.112995           1  \n",
       "2  0.020991           2  \n",
       "3  0.037081           2  \n",
       "4  0.083501           2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('./data/labelled_dataset.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(dataframe[\"Text\"])\n",
    "raw_corpus = list(dataframe[\"Raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import pprint\n",
    "# counts = dict(Counter(corpus))\n",
    "# duplicates = {key:value for key, value in counts.items() if value > 1}\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_col_names = [f\"{i}\" for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_scores(matrix:list, type:str)->dict:\n",
    "   \n",
    "   pair_scores = {}\n",
    "   res = []\n",
    "   if type=='similarity':\n",
    "      res = matrix.idxmax(axis='columns')\n",
    "   else:\n",
    "      res = matrix.idxmin(axis='columns')\n",
    "      \n",
    "   for index, column in enumerate(res):\n",
    "      \n",
    "      test = matrix.loc[matrix.index[index], column]\n",
    "      \n",
    "      if  test < .5 and test>=1:\n",
    "         continue\n",
    "         \n",
    "      doc1_len = len(corpus[index].split(' ')) \n",
    "      doc2_len = len(corpus[int(column)].split(' '))\n",
    "      # Exclude documents that are less than 4 and more than 30 characters\n",
    "      if doc1_len<=3 or doc2_len <=3 or doc1_len>=8 or doc2_len >=8:\n",
    "         continue\n",
    "      index = matrix.index[index]\n",
    "      if f\"{column}:{index}\" not in pair_scores.keys():\n",
    "         pair_scores[f\"{index}:{column}\"] = test\n",
    "   \n",
    "   return pair_scores\n",
    "\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    " \n",
    "def show_sentence_pairs(pairs:list):\n",
    "   for pair in pairs:\n",
    "      doc_pair_index = pair[0].split(':')\n",
    "      doc1_idx = int(doc_pair_index[0])\n",
    "      doc2_idx = int(doc_pair_index[1])\n",
    "\n",
    "      print(f\"Sentence 1: {raw_corpus[doc1_idx]} \\n Sentence 2: {raw_corpus[doc2_idx]}\")\n",
    "      if type == \"similarity\":\n",
    "         print(f\"Similarity Score:{pair[1]}\")\n",
    "      else:\n",
    "         print(f\"Dissimilarity Score:{pair[1]}\")\n",
    "      print('-----------------------------------------------------------------')\n",
    "\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = dataframe['Best Topic'].nunique()\n",
    "topic_index_dict = {}\n",
    "def get_indices_of_doc_with_similar_topic():\n",
    "   dict_var = {}\n",
    "   for i in range(n_topics):\n",
    "      dict_var[i]=dataframe.index[dataframe['Best Topic']==i+1].tolist()\n",
    "   return dict_var\n",
    "topic_index_dict = get_indices_of_doc_with_similar_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tf_model = tfidf.fit(corpus)\n",
    "tfidf_matrix = tf_model.transform(corpus)\n",
    "\n",
    "# Generate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4686</th>\n",
       "      <th>4687</th>\n",
       "      <th>4688</th>\n",
       "      <th>4689</th>\n",
       "      <th>4690</th>\n",
       "      <th>4691</th>\n",
       "      <th>4692</th>\n",
       "      <th>4693</th>\n",
       "      <th>4694</th>\n",
       "      <th>4695</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1         2         3         4    5         6    7         8  \\\n",
       "0       NaN  0.0  0.000000  0.127159  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "1  0.000000  NaN  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.330497   \n",
       "2  0.000000  0.0       NaN  0.000000  0.097896  0.0  0.038161  0.0  0.000000   \n",
       "3  0.127159  0.0  0.000000       NaN  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "4  0.000000  0.0  0.097896  0.000000       NaN  0.0  0.000000  0.0  0.000000   \n",
       "\n",
       "     9  ...  4686  4687  4688  4689      4690  4691      4692  4693  4694  \\\n",
       "0  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "1  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "2  0.0  ...   0.0   0.0   0.0   0.0  0.019566   0.0  0.047418   0.0   0.0   \n",
       "3  0.0  ...   0.0   0.0   0.0   0.0  0.150336   0.0  0.000000   0.0   0.0   \n",
       "4  0.0  ...   0.0   0.0   0.0   0.0  0.000000   0.0  0.000000   0.0   0.0   \n",
       "\n",
       "   4695  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 4696 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = pd.DataFrame(cosine_sim, index=row_col_names, columns=row_col_names )\n",
    "for i in range(len(similarity_matrix)): \n",
    "    similarity_matrix.iat[i, i] = np.nan\n",
    "similarity_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_similarity_df_by_topics = []\n",
    "def get_similarity_per_topics():\n",
    "   out =  []\n",
    "   for i in range(n_topics):\n",
    "      df1 = similarity_matrix.iloc[:,list(topic_index_dict[i])]\n",
    "      topicN_df = df1.iloc[list(topic_index_dict[i])]\n",
    "      out.append(topicN_df)\n",
    "   return out\n",
    "\n",
    "list_of_similarity_df_by_topics = get_similarity_per_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(list(topicN_df.iloc[:,0]))):\n",
    "#    if list(topicN_df.iloc[:,0])[i] != list(topicN_df.iloc[:,0])[i]:\n",
    "#       print(list(topicN_df.iloc[:,0])[i], list(topicN_df.iloc[:,0])[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity Score for Topic 1: 0.010447497075982528\n",
      "Average Similarity Score for Topic 2: 0.012150819435352966\n",
      "Average Similarity Score for Topic 3: 0.011244444354340223\n",
      "\n",
      "Topic 1 | Documents 1668\n",
      "Sentence 1: I think that I will have to… I will have to disagree with that number. We are not that high. - 88M \n",
      " Sentence 2: \"I think I will have to disagree with that number. We are not that high\"\n",
      "Dissimilarity Score:1.0000000000000002\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: \"I disagree with that number I think we're not that high\" \n",
      " Sentence 2: I think that I will have to… I will have to disagree with that number. We are not that high. - 88M\n",
      "Dissimilarity Score:1.0000000000000002\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: I think I will have to disagree with that number 🥴 \n",
      " Sentence 2: I think that I will have to… I will have to disagree with that number. We are not that high. - 88M\n",
      "Dissimilarity Score:0.8965798893896871\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Bakit iba ang sabi ng pulse asia??? \n",
      " Sentence 2: Pulse Asia or palsong asia.\n",
      "Dissimilarity Score:0.7947735645196916\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: \"TRUST ME BRO,\" \n",
      " Sentence 2: Source: Trust me bro moment\n",
      "Dissimilarity Score:0.7394168347125166\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 2 | Documents 2521\n",
      "Sentence 1: accelerate talaga, parang f1 car ba yan sa bilis \n",
      " Sentence 2: Ang Bilis ng pag accelerate ng inflation parang F1 lng\n",
      "Dissimilarity Score:0.8940873842304269\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Bagong Pilipinas \n",
      " Sentence 2: Bagong pilipinas bagong inflation hehehe0\n",
      "Dissimilarity Score:0.8591547634960202\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Hindi lang history ang gustong i-revise, pati pala inflation rate. \n",
      " Sentence 2: Pati inflation rate gustong i-revise\n",
      "Dissimilarity Score:0.8409116229398275\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: \"Tuloy na tuloy pa rin \n",
      " Sentence 2: sige tuloy tuloy lang mga 6yrs\n",
      "Dissimilarity Score:0.8209765905973414\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Salamat Sir. \n",
      " Sentence 2: Sir sir sir papataasin po ba natin? Salamat po sa unity\n",
      "Dissimilarity Score:0.76714392600891\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 3 | Documents 507\n",
      "Sentence 1: What steps? Foreign loans? \n",
      " Sentence 2: Anong steps? Party steps?\n",
      "Dissimilarity Score:0.4779900288676591\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Hi OP! Do you have any gossips about the incoming PSEI rebalancing? Like for example rumors about SECB,DMC,SCC? \n",
      "\n",
      "Another question, why the PSEI is always 30 stocks? Why not 35 so it can include more deserving companies like DMC or AREIT? \n",
      " Sentence 2: Hi. Any tsismis regarding PSEI rebalancing? Is it DMC or SCC?\n",
      "Dissimilarity Score:0.43849407881269875\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: what steps??? nothing has been done yet! \n",
      " Sentence 2: Anong steps? Party steps?\n",
      "Dissimilarity Score:0.4332622199156757\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Ay may steps undertaken pala? \n",
      " Sentence 2: Anong steps? Party steps?\n",
      "Dissimilarity Score:0.41360861817148376\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Did you read the news about the inflation and gdp target by year end? - and the interest rate? Yes they have plans- look it up \n",
      " Sentence 2: Philippines Gdp growth rate in q1 2022 is 6.5%-7.0%\n",
      "Dissimilarity Score:0.3456666256692608\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_topics):\n",
    "   print(f'Average Similarity Score for Topic {i+1}: {list_of_similarity_df_by_topics[i].iloc[:,0].mean()}')\n",
    "print('')\n",
    "for i in range(n_topics):\n",
    "   print(f'Topic {i+1} | Documents {list_of_similarity_df_by_topics[i].shape[0]}')\n",
    "   \n",
    "   pair_scores = get_pairs_scores(list_of_similarity_df_by_topics[i] , \"similarity\")\n",
    "   topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1), reverse=True))\n",
    "   topN_pairs = take(5, topPairs.items())\n",
    "   show_sentence_pairs(topN_pairs)\n",
    "   print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4686</th>\n",
       "      <th>4687</th>\n",
       "      <th>4688</th>\n",
       "      <th>4689</th>\n",
       "      <th>4690</th>\n",
       "      <th>4691</th>\n",
       "      <th>4692</th>\n",
       "      <th>4693</th>\n",
       "      <th>4694</th>\n",
       "      <th>4695</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.669503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.849664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1         2         3         4    5         6    7         8  \\\n",
       "0       NaN  1.0  1.000000  0.872841  1.000000  1.0  1.000000  1.0  1.000000   \n",
       "1  1.000000  NaN  1.000000  1.000000  1.000000  1.0  1.000000  1.0  0.669503   \n",
       "2  1.000000  1.0       NaN  1.000000  0.902104  1.0  0.961839  1.0  1.000000   \n",
       "3  0.872841  1.0  1.000000       NaN  1.000000  1.0  1.000000  1.0  1.000000   \n",
       "4  1.000000  1.0  0.902104  1.000000       NaN  1.0  1.000000  1.0  1.000000   \n",
       "\n",
       "     9  ...  4686  4687  4688  4689      4690  4691      4692  4693  4694  \\\n",
       "0  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "1  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "2  1.0  ...   1.0   1.0   1.0   1.0  0.980434   1.0  0.952582   1.0   1.0   \n",
       "3  1.0  ...   1.0   1.0   1.0   1.0  0.849664   1.0  1.000000   1.0   1.0   \n",
       "4  1.0  ...   1.0   1.0   1.0   1.0  1.000000   1.0  1.000000   1.0   1.0   \n",
       "\n",
       "   4695  \n",
       "0   1.0  \n",
       "1   1.0  \n",
       "2   1.0  \n",
       "3   1.0  \n",
       "4   1.0  \n",
       "\n",
       "[5 rows x 4696 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate cosine similarity\n",
    "cosine_distance = cosine_distances(tfidf_matrix, tfidf_matrix)\n",
    "distance_matrix = pd.DataFrame(cosine_distance, index=row_col_names, columns=row_col_names )\n",
    "for i in range(len(distance_matrix)): \n",
    "    distance_matrix.iat[i, i] = np.nan\n",
    "distance_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dissimilarity_df_by_topics = []\n",
    "def get_dissimilarity_per_topics():\n",
    "   out =  []\n",
    "   for i in range(n_topics):\n",
    "      df1 = distance_matrix.iloc[:,list(topic_index_dict[i])]\n",
    "      topicN_df = df1.iloc[list(topic_index_dict[i])]\n",
    "      out.append(topicN_df)\n",
    "   return out\n",
    "\n",
    "list_of_dissimilarity_df_by_topics = get_dissimilarity_per_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dissimilarity Score for Topic 1: 0.9895525029240174\n",
      "Average Dissimilarity Score for Topic 2: 0.9878491805646471\n",
      "Average Dissimilarity Score for Topic 3: 0.9887555556456598\n",
      "\n",
      "Topic 1 | Documents 1668\n",
      "Sentence 1: Solution: Ipa-recompute ang inflation, maging 3% nalang. Haha.\n",
      "\n",
      "Naalala ko si Ate Glo before, para hindi magmukhang kulang ng classrooms, pinadivide into 2 ang classroom requirement kasi yun assumption niya na 2 shifts per classroom sa lahat ng schools. Ito yung link\n",
      "\n",
      "https://www.gmanetwork.com/news/topstories/nation/7405/public-schools-reveal-acute-classroom-shortage/story/ \n",
      " Sentence 2: 6% per month is 70% per annum, dating 100 maging 170 in 12months...\n",
      "Dissimilarity Score:0.9119746953976552\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Inflation is growing bad, BBM just made it look like a sexy woman with growing bust and butt size roaming on the street looking for rich people, and he fell in love with that ugly bish. \n",
      " Sentence 2: Too bad for him, and if it he doesn't do anything about it, too bad for the PH\n",
      "Dissimilarity Score:0.8775788103633342\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: You got scammed by LeniLugaw and now you're going to get scammed by butters lmao \n",
      " Sentence 2: Lmao you are in the Philippines looking for authentic Mexican food? Are you good OP? 😂\n",
      "Dissimilarity Score:0.8615762521009439\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Fellow kakampinks, we may fall to the \"ginusto niyo yan\" argument but as much as we want everyone to have the same perspective as we have, we can't enforce change unless they want it. Alam Kong nakakainis at nakaka frustrate na nadadamay din tayo sa ka… \n",
      " Sentence 2: Some people just don't want to be saved\n",
      "Dissimilarity Score:0.8596515789685267\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Damn.\n",
      "\n",
      "I always knew about the traffic situation in Manila and streets having no street lights outside of the big cities having traveled there before. I was thinking about moving there as a FIRE Expat. Safety is a big concern of mine and the lack of urgency displayed there would get old real quick. \n",
      "\n",
      "Are there other countries in Southeast Asia that are more attractive for prospective expats? Thailand and Vietnam are supposedly great places to retire to. \n",
      " Sentence 2: I totally agree with the part about the lack of urgency.\n",
      "Dissimilarity Score:0.8574643437818015\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 2 | Documents 2521\n",
      "Sentence 1: Thatcia a part of their agenda, \"THE ECONOMIC SABOTAGE\"!!! \n",
      " Sentence 2: Wala pa tayo sa ~~exciting~~ exhausting part\n",
      "Dissimilarity Score:0.8769114592148836\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: government starterpack \n",
      " Sentence 2: GINUSTO NYO YAN. DEAL WITH IT. YOU DESERVED THIS GOVERNMENT. PERIODT.\n",
      "Dissimilarity Score:0.868804759907323\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Hindi ko na alam kung gaano pa karaming effort ang mabibigay ko. Lahat ng pagtitipid at pagtitiis ginagawa ko. Nagsisipag ako pero parang walang nararating. Bwisit talaga. \n",
      " Sentence 2: Grabe kasi yung dami ng anak !! Sa tuwing nakikita ko to naiirita ko !! Bwisit\n",
      "Dissimilarity Score:0.868763415916116\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: infact ginagawa na sa ilocos yan,meron na silang mga advance way of farming,farming machinery.hybrid rice.aquaponicsvegetable farming,fertilizer drone \n",
      " Sentence 2: pareho silang high nung Inflation Rate\n",
      "Dissimilarity Score:0.860368130397416\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Mao n ky way pakabana ang presidente....mka sad kaayo... maayo pa ug wa ko ni boto sa iya... \n",
      " Sentence 2: Boto nyo ako sa susunod na eleksyon para maiba naman...\n",
      "Dissimilarity Score:0.8563289128799701\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "Topic 3 | Documents 507\n",
      "Sentence 1: ganyan ba talaga yung plano because that still doesn't seem feasible business-wise \n",
      " Sentence 2: Thanks for this info. Most old people and our parents doesn't know this 🙄\n",
      "Dissimilarity Score:0.9002174030351145\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Putangina ng basurang angkan na yan, dapat talaga banned na kahit 5 generations ng mga kupal na yan from tthe time they were exiled \n",
      " Sentence 2: Will not be surprised at all if we're apocalyptic or dystopian a generation or two from now.\n",
      "Dissimilarity Score:0.8924394895159091\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Doesn’t BSP’s approach to this sound like Laissez-Faire? \n",
      " Sentence 2: Thanks for this info. Most old people and our parents doesn't know this 🙄\n",
      "Dissimilarity Score:0.8862889439346084\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: Just a little insight for JGO, Ethylene plant was supposed to start mid-July but was extended a couple of times due to failure of key equipment. It should be up and running by the 4th week of August if no other issues pop up. \n",
      " Sentence 2: Isn't the price supposed to be the same for both stock exchanges?\n",
      "Dissimilarity Score:0.8797470848864001\n",
      "-----------------------------------------------------------------\n",
      "Sentence 1: For anyone that also receives the email newsletter, please add the new delivery address (\"mb@merkadobarkada.com\") to your Contacts list. Been having some trouble with the gmail spam filters last week! Thank you! \n",
      " Sentence 2: This is so good! Do you guys have an email subscription?\n",
      "Dissimilarity Score:0.8713176975272697\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_topics):\n",
    "   print(f'Average Dissimilarity Score for Topic {i+1}: {list_of_dissimilarity_df_by_topics[i].iloc[:,0].mean()}')\n",
    "print('')\n",
    "for i in range(n_topics):\n",
    "   print(f'Topic {i+1} | Documents {list_of_dissimilarity_df_by_topics[i].shape[0]}')\n",
    "   \n",
    "   pair_scores = get_pairs_scores(list_of_dissimilarity_df_by_topics[i] , \"dissimilarity\")\n",
    "   topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1), reverse=True))\n",
    "   topN_pairs = take(5, topPairs.items())\n",
    "   show_sentence_pairs(topN_pairs)\n",
    "   print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res= topicN_df.idxmax(axis='columns')\n",
    "# print(res)\n",
    "# maxes = []\n",
    "# test_shit = []\n",
    "# for index, column in enumerate(res):\n",
    "#    test_shit.append(int(column))\n",
    "#    test = topicN_df.loc[topicN_df.index[index], column]\n",
    "#    maxes.append(test)\n",
    "# print(mean(maxes))\n",
    "# print(len(maxes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pair_scores = get_pairs_scores(distance_matrix, \"distance\")\n",
    "# topPairs = dict(sorted(pair_scores.items(), key=operator.itemgetter(1),reverse=True))\n",
    "# topN_pairs = take(20, topPairs.items())\n",
    "# show_sentence_pairs(topN_pairs)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d7e9679791a0cc8fec0355bccbe56b27315ba347fb40fe2eed38daf52cf00c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
