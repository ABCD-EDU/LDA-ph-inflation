{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Preprocessing\n",
    "import re\n",
    "import preprocessor as p\n",
    "import string\n",
    "import contractions\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cgab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cgab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\cgab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cgab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cgab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download requires corpus\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Consolidation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# Consolidate all csvs into one dataframe then export as csv\n",
    "fb_df = pd.read_csv('./data/social_media/Facebook_Consolidated.csv')\n",
    "fb_df[\"platform\"] = \"Facebook\"\n",
    "reddit_df = pd.read_csv('./data/social_media/reddit.csv')\n",
    "reddit_df[\"platform\"] = \"Reddit\"\n",
    "tiktok_df = pd.read_excel('./data/social_media/Tiktok.xlsx')\n",
    "tiktok_df[\"platform\"] = \"Tiktok\"\n",
    "youtube_df = pd.read_csv('./data/social_media/Youtube.csv')\n",
    "youtube_df[\"platform\"] = \"Youtube\"\n",
    "# Create master_series\n",
    "text = fb_df[\"0\"].append(reddit_df[\"body\"], ignore_index=True).append(tiktok_df[\"Comment Text\"], ignore_index=True).append(youtube_df[\"Comment\"], ignore_index=True)\n",
    "platform = fb_df[\"platform\"].append(reddit_df[\"platform\"], ignore_index=True).append(tiktok_df[\"platform\"], ignore_index=True).append(youtube_df[\"platform\"], ignore_index=True)\n",
    "# Create master_df\n",
    "master_df = pd.DataFrame({\"text\": text, \"platform\": platform})\n",
    "# Drop master_df NaN\n",
    "master_df = master_df[master_df['text'].notna()]\n",
    "master_df.to_csv('./data/consolidated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing </h2>\n",
    "TODO: Remove entries that have only 1 word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/consolidated.csv')\n",
    "dataset=dataset.rename(columns = {'text':'raw'})\n",
    "dataset = dataset[[\"platform\", \"raw\"]]\n",
    "# dataset = pd.read_csv('./data/sample.csv')\n",
    "# dataset.columns = ['raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column for prepr preprocessed\n",
    "dataset[\"preprocessed\"] = dataset[\"raw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>raw</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Why nowadays every thing  seem to be increasin...</td>\n",
       "      <td>Why nowadays every thing  seem to be increasin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>I will have to disagree.. we’re not that high!!</td>\n",
       "      <td>I will have to disagree.. we’re not that high!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Wag po tayong mag-alala. Naniniwala po ako, is...</td>\n",
       "      <td>Wag po tayong mag-alala. Naniniwala po ako, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Ok lang yang lahat naman nang bansa ganyan. Sa...</td>\n",
       "      <td>Ok lang yang lahat naman nang bansa ganyan. Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Sama-sama tayong BABAON muli.</td>\n",
       "      <td>Sama-sama tayong BABAON muli.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   platform                                                raw  \\\n",
       "0  Facebook  Why nowadays every thing  seem to be increasin...   \n",
       "1  Facebook    I will have to disagree.. we’re not that high!!   \n",
       "2  Facebook  Wag po tayong mag-alala. Naniniwala po ako, is...   \n",
       "3  Facebook  Ok lang yang lahat naman nang bansa ganyan. Sa...   \n",
       "4  Facebook                      Sama-sama tayong BABAON muli.   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  Why nowadays every thing  seem to be increasin...  \n",
       "1    I will have to disagree.. we’re not that high!!  \n",
       "2  Wag po tayong mag-alala. Naniniwala po ako, is...  \n",
       "3  Ok lang yang lahat naman nang bansa ganyan. Sa...  \n",
       "4                      Sama-sama tayong BABAON muli.  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "# include filipino stopwords\n",
    "f = open(\"./data/stopwords.txt\", \"r\")\n",
    "for line in f:\n",
    "    stop.add(line.strip())\n",
    "f = open(\"./data/stopwords2.txt\", \"r\")\n",
    "for line in f:\n",
    "    stop.add(line.strip())\n",
    "f.close()\n",
    "\n",
    "# Helper functions\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stop]\n",
    "    return words\n",
    "\n",
    "def lemmatize_text(tokenized_text):\n",
    "    return [lemma.lemmatize(w) for w in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5209"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "dataset.drop_duplicates(keep = False, inplace = True)\n",
    "# Convert to lowercase\n",
    "dataset[\"preprocessed\"] = [entry.lower() for entry in dataset[\"preprocessed\"]]\n",
    "# Remove punctuation marks\n",
    "dataset[\"preprocessed\"] = [entry.translate(str.maketrans('', '', string.punctuation)) for entry in dataset[\"preprocessed\"]]\n",
    "# Remove numbers\n",
    "dataset[\"preprocessed\"] = [re.sub(r'\\d+', '', entry) for entry in dataset[\"preprocessed\"]]\n",
    "# Twitter preprocess\n",
    "dataset[\"preprocessed\"] = [p.clean(entry) for entry in dataset[\"preprocessed\"]]\n",
    "# Tokenization\n",
    "dataset['preprocessed']= [word_tokenize(entry) for entry in dataset['preprocessed']]\n",
    "# Normalizatinon\n",
    "dataset['preprocessed'] = dataset['preprocessed'].apply(lemmatize_text)\n",
    "# Remove stopwords\n",
    "dataset[\"preprocessed\"] = [remove_stopwords(entry) for entry in dataset[\"preprocessed\"]]\n",
    "# Remove NA\n",
    "dataset.dropna(inplace=True)\n",
    "# Drop rows with preprocessed tokens length less than 2\n",
    "dataset[\"length\"] = [len(entry) for entry in dataset[\"preprocessed\"]]\n",
    "dataset = dataset[dataset[\"length\"] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4725"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>raw</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Why nowadays every thing  seem to be increasin...</td>\n",
       "      <td>[nowadays, every, thing, seem, increasing, gov...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>I will have to disagree.. we’re not that high!!</td>\n",
       "      <td>[disagree, high]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Wag po tayong mag-alala. Naniniwala po ako, is...</td>\n",
       "      <td>[wag, tayong, magalala, naniniwala, isusuprise...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Ok lang yang lahat naman nang bansa ganyan. Sa...</td>\n",
       "      <td>[ok, yang, nang, bansa, ganyan, selfish, fanat...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Sama-sama tayong BABAON muli.</td>\n",
       "      <td>[samasama, tayong, babaon]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>mukhang nakashabu</td>\n",
       "      <td>[mukhang, nakashabu]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>Bbm is mixed up and confusing on economic terms</td>\n",
       "      <td>[bbm, mixed, confusing, economic, term]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>Mr.utal utal</td>\n",
       "      <td>[mrutal, utal]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>Kung Si Leni Pa Yan \\r\\n\\r\\nLutang Na Tayo</td>\n",
       "      <td>[leni, lutang]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>he knowns what to do wag kayo puro kuda wala n...</td>\n",
       "      <td>[knowns, wag, puro, kuda, wala, maitutulong, m...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4725 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      platform                                                raw  \\\n",
       "0     Facebook  Why nowadays every thing  seem to be increasin...   \n",
       "1     Facebook    I will have to disagree.. we’re not that high!!   \n",
       "2     Facebook  Wag po tayong mag-alala. Naniniwala po ako, is...   \n",
       "3     Facebook  Ok lang yang lahat naman nang bansa ganyan. Sa...   \n",
       "4     Facebook                      Sama-sama tayong BABAON muli.   \n",
       "...        ...                                                ...   \n",
       "5204   Youtube                                  mukhang nakashabu   \n",
       "5205   Youtube    Bbm is mixed up and confusing on economic terms   \n",
       "5206   Youtube                                       Mr.utal utal   \n",
       "5207   Youtube        Kung Si Leni Pa Yan \\r\\n\\r\\nLutang Na Tayo    \n",
       "5208   Youtube  he knowns what to do wag kayo puro kuda wala n...   \n",
       "\n",
       "                                           preprocessed  length  \n",
       "0     [nowadays, every, thing, seem, increasing, gov...      17  \n",
       "1                                      [disagree, high]       2  \n",
       "2     [wag, tayong, magalala, naniniwala, isusuprise...      16  \n",
       "3     [ok, yang, nang, bansa, ganyan, selfish, fanat...       8  \n",
       "4                            [samasama, tayong, babaon]       3  \n",
       "...                                                 ...     ...  \n",
       "5204                               [mukhang, nakashabu]       2  \n",
       "5205            [bbm, mixed, confusing, economic, term]       5  \n",
       "5206                                     [mrutal, utal]       2  \n",
       "5207                                     [leni, lutang]       2  \n",
       "5208  [knowns, wag, puro, kuda, wala, maitutulong, m...      22  \n",
       "\n",
       "[4725 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('./data/preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed = pd.read_csv('./data/preprocessed.csv')\n",
    "# df = pd.DataFrame({\"raw\":dataset[\"text\"], \"preprocessed\":preprocessed[\"text\"]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
